{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-30T05:36:44.113709Z",
     "start_time": "2024-05-30T05:36:44.038337Z"
    }
   },
   "source": "import PyPDF2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:39:34.932364Z",
     "start_time": "2024-05-30T05:39:34.911343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "myfile = open('meta-learning.pdf',mode='rb')\n",
    "pdf_reader = PyPDF2.PdfReader(myfile)\n",
    "len(pdf_reader.pages)"
   ],
   "id": "9d766fd0fe88b8ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:40:09.461875Z",
     "start_time": "2024-05-30T05:40:09.453868Z"
    }
   },
   "cell_type": "code",
   "source": "page_one = pdf_reader.pages[0]",
   "id": "1d4739cf4455867",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:40:18.510440Z",
     "start_time": "2024-05-30T05:40:18.450812Z"
    }
   },
   "cell_type": "code",
   "source": "page_one.extract_text()",
   "id": "cbbf7c8e33e69680",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meta-Learning Framework with Applications to Zero-Shot Time-Series\\nForecasting\\nBoris N. Oreshkin1, Dmitri Carpov1, Nicolas Chapados1, Yoshua Bengio2\\n1Element AI,2Mila\\nboris.oreshkin@gmail.com\\nAbstract\\nCan meta-learning discover generic ways of processing time\\nseries (TS) from a diverse dataset so as to greatly improve\\ngeneralization on new TS coming from different datasets?\\nThis work provides positive evidence to this using a broad\\nmeta-learning framework which we show subsumes many\\nexisting meta-learning algorithms. Our theoretical analysis\\nsuggests that residual connections act as a meta-learning adap-\\ntation mechanism, generating a subset of task-speciﬁc param-\\neters based on a given TS input, thus gradually expanding\\nthe expressive power of the architecture on-the-ﬂy. The same\\nmechanism is shown via linearization analysis to have the\\ninterpretation of a sequential update of the ﬁnal linear layer.\\nOur empirical results on a wide range of data emphasize the\\nimportance of the identiﬁed meta-learning mechanisms for\\nsuccessful zero-shot univariate forecasting, suggesting that it\\nis viable to train a neural network on a source TS dataset and\\ndeploy it on a different target TS dataset without retraining,\\nresulting in performance that is at least as good as that of\\nstate-of-practice univariate forecasting models.\\n1 Introduction\\nTime series (TS) forecasting is both a fundamental scientiﬁc\\nproblem and one of great practical importance. It is central\\nto the actions of intelligent agents: the ability to plan and\\ncontrol as well as to appropriately react to manifestations\\nof complex partially or completely unknown systems often\\nrelies on the ability to forecast relevant observations based on\\npast history. Moreover, for most utility-maximizing agents,\\ngains in forecasting accuracy broadly translate into utility\\ngains; as such, improvements in forecasting technology can\\nhave wide impacts. Unsurprisingly, forecasting methods have\\na long history that can be traced back to the very origins\\nof human civilization (Neale 1985), modern science (Gauss\\n1809) and have consistently attracted considerable research\\nattention (Yule 1927; Walker 1931; Holt 1957; Winters 1960;\\nEngle 1982; Sezer, Gudelek, and Ozbayoglu 2019). The appli-\\ncations of forecasting span a variety of ﬁelds, including high-\\nfrequency control (e.g. vehicle and robot control (Tang and\\nSalakhutdinov 2019), data center optimization (Gao 2014)),\\nbusiness planning (supply chain management (Leung 1995),\\nworkforce and call center management (Chapados et al. 2014;\\nCopyright c\\r2021, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.Ibrahim et al. 2016), as well as such critically important areas\\nas precision agriculture (Rodrigues Jr et al. 2019). In business\\nspeciﬁcally, improved forecasting translates in better pro-\\nduction planning (leading to less waste) and less transporta-\\ntion (reducing CO 2emissions) (Kahn 2003; Kerkkänen, Kor-\\npela, and Huiskonen 2009; Nguyen, Ni, and Rossetti 2010).\\nThe progress made in univariate forecasting in the past four\\ndecades is well reﬂected in the results and methods consid-\\nered in associated competitions over that period (Makridakis\\net al. 1982, 1993; Makridakis and Hibon 2000; Athanasopou-\\nlos et al. 2011; Makridakis, Spiliotis, and Assimakopoulos\\n2018a). Recently, growing evidence has started to emerge\\nsuggesting that machine learning approaches could improve\\non classical forecasting methods, in contrast to some ear-\\nlier assessments (Makridakis, Spiliotis, and Assimakopoulos\\n2018b). For example, the winner of the 2018 M4 competition\\n(Makridakis, Spiliotis, and Assimakopoulos 2018a) was a\\nneural network designed by Smyl (2020).\\nOn the practical side, the deployment of deep neural time-\\nseries models is challenged by the cold start problem. Before\\natabula rasa deep neural network provides a useful forecast-\\ning output, it should be trained on a large problem-speciﬁc\\ntime-series dataset. For early adopters, this often implies data\\ncollection efforts, changing data handling practices and even\\nchanging the existing IT infrastructures on a large scale. In\\ncontrast, advanced statistical models can be deployed with\\nsigniﬁcantly less effort as they estimate their parameters on\\nsingle time series at a time. In this paper we address the prob-\\nlem of reducing the entry cost of deep neural networks in\\nthe industrial practice of TS forecasting. We show that it is\\nviable to train a neural network model on a diversiﬁed source\\ndataset and deploy it on a target dataset in a zero-shot regime,\\ni.e. without explicit retraining on that target data, resulting\\nin performance that is at least as good as that of advanced\\nstatistical models tailored to the target dataset. We would like\\nto clarify that we use the term “zero-shot” in our work in the\\nsense that the number of history samples available for the\\ntarget time series is so small that it makes training a deep\\nlearning model on this time series infeasible.\\nAddressing this practical problem provides clues to fun-\\ndamental questions. Can we learn something general about\\nforecasting and transfer this knowledge across datasets? If\\nso, what kind of mechanisms could facilitate this? The abil-\\nity to learn and transfer representations across tasks via\\nTheThirty-FifthAAAIConferenceonArtificial Intelligence(AAAI-21)\\n9242'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:41:05.182017Z",
     "start_time": "2024-05-30T05:41:05.171007Z"
    }
   },
   "cell_type": "code",
   "source": "myfile.close()",
   "id": "580b0e277ba27776",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:42:21.483568Z",
     "start_time": "2024-05-30T05:42:21.468552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f = open('meta-learning.pdf',mode='rb')\n",
    "pdf_reader = PyPDF2.PdfReader(f)"
   ],
   "id": "778fb56a11e5f45a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:42:36.471600Z",
     "start_time": "2024-05-30T05:42:36.457591Z"
    }
   },
   "cell_type": "code",
   "source": "first_page = pdf_reader.pages[0]",
   "id": "f4482d008aaaf533",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:43:03.809595Z",
     "start_time": "2024-05-30T05:43:03.793579Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_writer = PyPDF2.PdfWriter()",
   "id": "54d24d12df4f6267",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:44:56.117362Z",
     "start_time": "2024-05-30T05:44:56.019270Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_writer.add_page(first_page)",
   "id": "a9511907287fab84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Contents': [IndirectObject(5, 0, 2312885861632),\n",
       "  IndirectObject(6, 0, 2312885861632),\n",
       "  IndirectObject(7, 0, 2312885861632),\n",
       "  IndirectObject(8, 0, 2312885861632),\n",
       "  IndirectObject(9, 0, 2312885861632),\n",
       "  IndirectObject(10, 0, 2312885861632),\n",
       "  IndirectObject(11, 0, 2312885861632),\n",
       "  IndirectObject(12, 0, 2312885861632)],\n",
       " '/CropBox': [0, 0, 612, 792],\n",
       " '/MediaBox': [0, 0, 612, 792],\n",
       " '/Resources': {'/Font': {'/T1_0': {'/BaseFont': '/VQOVOK+NimbusRomNo9L-Medi',\n",
       "    '/Encoding': {'/Differences': [2,\n",
       "      '/fi',\n",
       "      '/fl',\n",
       "      37,\n",
       "      '/percent',\n",
       "      '/ampersand',\n",
       "      '/quoteright',\n",
       "      '/parenleft',\n",
       "      '/parenright',\n",
       "      44,\n",
       "      '/comma',\n",
       "      '/hyphen',\n",
       "      '/period',\n",
       "      '/slash',\n",
       "      '/zero',\n",
       "      '/one',\n",
       "      '/two',\n",
       "      '/three',\n",
       "      '/four',\n",
       "      '/five',\n",
       "      '/six',\n",
       "      '/seven',\n",
       "      '/eight',\n",
       "      '/nine',\n",
       "      '/colon',\n",
       "      '/semicolon',\n",
       "      63,\n",
       "      '/question',\n",
       "      '/at',\n",
       "      '/A',\n",
       "      '/B',\n",
       "      '/C',\n",
       "      '/D',\n",
       "      '/E',\n",
       "      '/F',\n",
       "      '/G',\n",
       "      '/H',\n",
       "      '/I',\n",
       "      '/J',\n",
       "      '/K',\n",
       "      '/L',\n",
       "      '/M',\n",
       "      '/N',\n",
       "      '/O',\n",
       "      '/P',\n",
       "      '/Q',\n",
       "      '/R',\n",
       "      '/S',\n",
       "      '/T',\n",
       "      '/U',\n",
       "      '/V',\n",
       "      '/W',\n",
       "      '/X',\n",
       "      '/Y',\n",
       "      '/Z',\n",
       "      97,\n",
       "      '/a',\n",
       "      '/b',\n",
       "      '/c',\n",
       "      '/d',\n",
       "      '/e',\n",
       "      '/f',\n",
       "      '/g',\n",
       "      '/h',\n",
       "      '/i',\n",
       "      '/j',\n",
       "      '/k',\n",
       "      '/l',\n",
       "      '/m',\n",
       "      '/n',\n",
       "      '/o',\n",
       "      '/p',\n",
       "      '/q',\n",
       "      '/r',\n",
       "      '/s',\n",
       "      '/t',\n",
       "      '/u',\n",
       "      '/v',\n",
       "      '/w',\n",
       "      '/x',\n",
       "      '/y',\n",
       "      '/z',\n",
       "      134,\n",
       "      '/dagger',\n",
       "      '/daggerdbl',\n",
       "      147,\n",
       "      '/quotedblleft',\n",
       "      '/quotedblright',\n",
       "      150,\n",
       "      '/endash',\n",
       "      '/emdash',\n",
       "      201,\n",
       "      '/Eacute',\n",
       "      228,\n",
       "      '/adieresis',\n",
       "      239,\n",
       "      '/idieresis',\n",
       "      252,\n",
       "      '/udieresis'],\n",
       "     '/Type': '/Encoding'},\n",
       "    '/FirstChar': 2,\n",
       "    '/FontDescriptor': {'/Ascent': 690,\n",
       "     '/CapHeight': 690,\n",
       "     '/Descent': -209,\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-168, -341, 1000, 960],\n",
       "     '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "     '/FontName': '/VQOVOK+NimbusRomNo9L-Medi',\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 140,\n",
       "     '/Type': '/FontDescriptor',\n",
       "     '/XHeight': 461},\n",
       "    '/LastChar': 124,\n",
       "    '/Subtype': '/Type1',\n",
       "    '/Type': '/Font',\n",
       "    '/Widths': [556,\n",
       "     556,\n",
       "     167,\n",
       "     333,\n",
       "     667,\n",
       "     278,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     333,\n",
       "     570,\n",
       "     0,\n",
       "     667,\n",
       "     444,\n",
       "     333,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     278,\n",
       "     250,\n",
       "     333,\n",
       "     555,\n",
       "     500,\n",
       "     500,\n",
       "     1000,\n",
       "     833,\n",
       "     333,\n",
       "     333,\n",
       "     333,\n",
       "     500,\n",
       "     570,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     333,\n",
       "     570,\n",
       "     570,\n",
       "     570,\n",
       "     500,\n",
       "     930,\n",
       "     722,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     667,\n",
       "     611,\n",
       "     778,\n",
       "     778,\n",
       "     389,\n",
       "     500,\n",
       "     778,\n",
       "     667,\n",
       "     944,\n",
       "     722,\n",
       "     778,\n",
       "     611,\n",
       "     778,\n",
       "     722,\n",
       "     556,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     1000,\n",
       "     722,\n",
       "     722,\n",
       "     667,\n",
       "     333,\n",
       "     278,\n",
       "     333,\n",
       "     581,\n",
       "     500,\n",
       "     333,\n",
       "     500,\n",
       "     556,\n",
       "     444,\n",
       "     556,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     556,\n",
       "     278,\n",
       "     333,\n",
       "     556,\n",
       "     278,\n",
       "     833,\n",
       "     556,\n",
       "     500,\n",
       "     556,\n",
       "     556,\n",
       "     444,\n",
       "     389,\n",
       "     333,\n",
       "     556,\n",
       "     500,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0]},\n",
       "   '/T1_1': {'/BaseFont': '/LARLZF+NimbusRomNo9L-Regu',\n",
       "    '/Encoding': {'/Differences': [2,\n",
       "      '/fi',\n",
       "      '/fl',\n",
       "      37,\n",
       "      '/percent',\n",
       "      '/ampersand',\n",
       "      '/quoteright',\n",
       "      '/parenleft',\n",
       "      '/parenright',\n",
       "      44,\n",
       "      '/comma',\n",
       "      '/hyphen',\n",
       "      '/period',\n",
       "      '/slash',\n",
       "      '/zero',\n",
       "      '/one',\n",
       "      '/two',\n",
       "      '/three',\n",
       "      '/four',\n",
       "      '/five',\n",
       "      '/six',\n",
       "      '/seven',\n",
       "      '/eight',\n",
       "      '/nine',\n",
       "      '/colon',\n",
       "      '/semicolon',\n",
       "      63,\n",
       "      '/question',\n",
       "      '/at',\n",
       "      '/A',\n",
       "      '/B',\n",
       "      '/C',\n",
       "      '/D',\n",
       "      '/E',\n",
       "      '/F',\n",
       "      '/G',\n",
       "      '/H',\n",
       "      '/I',\n",
       "      '/J',\n",
       "      '/K',\n",
       "      '/L',\n",
       "      '/M',\n",
       "      '/N',\n",
       "      '/O',\n",
       "      '/P',\n",
       "      '/Q',\n",
       "      '/R',\n",
       "      '/S',\n",
       "      '/T',\n",
       "      '/U',\n",
       "      '/V',\n",
       "      '/W',\n",
       "      '/X',\n",
       "      '/Y',\n",
       "      '/Z',\n",
       "      97,\n",
       "      '/a',\n",
       "      '/b',\n",
       "      '/c',\n",
       "      '/d',\n",
       "      '/e',\n",
       "      '/f',\n",
       "      '/g',\n",
       "      '/h',\n",
       "      '/i',\n",
       "      '/j',\n",
       "      '/k',\n",
       "      '/l',\n",
       "      '/m',\n",
       "      '/n',\n",
       "      '/o',\n",
       "      '/p',\n",
       "      '/q',\n",
       "      '/r',\n",
       "      '/s',\n",
       "      '/t',\n",
       "      '/u',\n",
       "      '/v',\n",
       "      '/w',\n",
       "      '/x',\n",
       "      '/y',\n",
       "      '/z',\n",
       "      134,\n",
       "      '/dagger',\n",
       "      '/daggerdbl',\n",
       "      147,\n",
       "      '/quotedblleft',\n",
       "      '/quotedblright',\n",
       "      150,\n",
       "      '/endash',\n",
       "      '/emdash',\n",
       "      201,\n",
       "      '/Eacute',\n",
       "      228,\n",
       "      '/adieresis',\n",
       "      239,\n",
       "      '/idieresis',\n",
       "      252,\n",
       "      '/udieresis'],\n",
       "     '/Type': '/Encoding'},\n",
       "    '/FirstChar': 2,\n",
       "    '/FontDescriptor': {'/Ascent': 678,\n",
       "     '/CapHeight': 651,\n",
       "     '/Descent': -216,\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-168, -281, 1000, 924],\n",
       "     '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "     '/FontName': '/LARLZF+NimbusRomNo9L-Regu',\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 85,\n",
       "     '/Type': '/FontDescriptor',\n",
       "     '/XHeight': 450},\n",
       "    '/LastChar': 254,\n",
       "    '/Subtype': '/Type1',\n",
       "    '/Type': '/Font',\n",
       "    '/Widths': [556,\n",
       "     556,\n",
       "     167,\n",
       "     333,\n",
       "     611,\n",
       "     278,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     333,\n",
       "     564,\n",
       "     0,\n",
       "     611,\n",
       "     444,\n",
       "     333,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     180,\n",
       "     250,\n",
       "     333,\n",
       "     408,\n",
       "     500,\n",
       "     500,\n",
       "     833,\n",
       "     778,\n",
       "     333,\n",
       "     333,\n",
       "     333,\n",
       "     500,\n",
       "     564,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     564,\n",
       "     564,\n",
       "     564,\n",
       "     444,\n",
       "     921,\n",
       "     722,\n",
       "     667,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     556,\n",
       "     722,\n",
       "     722,\n",
       "     333,\n",
       "     389,\n",
       "     722,\n",
       "     611,\n",
       "     889,\n",
       "     722,\n",
       "     722,\n",
       "     556,\n",
       "     722,\n",
       "     667,\n",
       "     556,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     944,\n",
       "     722,\n",
       "     722,\n",
       "     611,\n",
       "     333,\n",
       "     278,\n",
       "     333,\n",
       "     469,\n",
       "     500,\n",
       "     333,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     500,\n",
       "     278,\n",
       "     778,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     480,\n",
       "     200,\n",
       "     480,\n",
       "     541,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     500,\n",
       "     444,\n",
       "     1000,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     1000,\n",
       "     556,\n",
       "     333,\n",
       "     889,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     444,\n",
       "     350,\n",
       "     500,\n",
       "     1000,\n",
       "     333,\n",
       "     980,\n",
       "     389,\n",
       "     333,\n",
       "     722,\n",
       "     0,\n",
       "     0,\n",
       "     722,\n",
       "     0,\n",
       "     333,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     200,\n",
       "     500,\n",
       "     333,\n",
       "     760,\n",
       "     276,\n",
       "     500,\n",
       "     564,\n",
       "     333,\n",
       "     760,\n",
       "     333,\n",
       "     400,\n",
       "     564,\n",
       "     300,\n",
       "     300,\n",
       "     333,\n",
       "     500,\n",
       "     453,\n",
       "     250,\n",
       "     333,\n",
       "     300,\n",
       "     310,\n",
       "     500,\n",
       "     750,\n",
       "     750,\n",
       "     750,\n",
       "     444,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     889,\n",
       "     667,\n",
       "     611,\n",
       "     611,\n",
       "     611,\n",
       "     611,\n",
       "     333,\n",
       "     333,\n",
       "     333,\n",
       "     333,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     564,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     722,\n",
       "     556,\n",
       "     500,\n",
       "     444,\n",
       "     444,\n",
       "     444,\n",
       "     444,\n",
       "     444,\n",
       "     444,\n",
       "     667,\n",
       "     444,\n",
       "     444,\n",
       "     444,\n",
       "     444,\n",
       "     444,\n",
       "     278,\n",
       "     278,\n",
       "     278,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     564,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     0]},\n",
       "   '/T1_2': {'/BaseFont': '/WEIJUK+CMSY9',\n",
       "    '/FirstChar': 13,\n",
       "    '/FontDescriptor': {'/Ascent': 750,\n",
       "     '/CapHeight': 683,\n",
       "     '/Descent': -194,\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-29, -958, 1146, 777],\n",
       "     '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "     '/FontName': '/WEIJUK+CMSY9',\n",
       "     '/ItalicAngle': -14,\n",
       "     '/StemV': 43,\n",
       "     '/Type': '/FontDescriptor',\n",
       "     '/XHeight': 431},\n",
       "    '/LastChar': 15,\n",
       "    '/Subtype': '/Type1',\n",
       "    '/Type': '/Font',\n",
       "    '/Widths': [1027, 0, 0]},\n",
       "   '/T1_3': {'/BaseFont': '/DJQNQL+NimbusRomNo9L-ReguItal',\n",
       "    '/Encoding': {'/Differences': [2,\n",
       "      '/fi',\n",
       "      '/fl',\n",
       "      37,\n",
       "      '/percent',\n",
       "      '/ampersand',\n",
       "      '/quoteright',\n",
       "      '/parenleft',\n",
       "      '/parenright',\n",
       "      44,\n",
       "      '/comma',\n",
       "      '/hyphen',\n",
       "      '/period',\n",
       "      '/slash',\n",
       "      '/zero',\n",
       "      '/one',\n",
       "      '/two',\n",
       "      '/three',\n",
       "      '/four',\n",
       "      '/five',\n",
       "      '/six',\n",
       "      '/seven',\n",
       "      '/eight',\n",
       "      '/nine',\n",
       "      '/colon',\n",
       "      '/semicolon',\n",
       "      63,\n",
       "      '/question',\n",
       "      '/at',\n",
       "      '/A',\n",
       "      '/B',\n",
       "      '/C',\n",
       "      '/D',\n",
       "      '/E',\n",
       "      '/F',\n",
       "      '/G',\n",
       "      '/H',\n",
       "      '/I',\n",
       "      '/J',\n",
       "      '/K',\n",
       "      '/L',\n",
       "      '/M',\n",
       "      '/N',\n",
       "      '/O',\n",
       "      '/P',\n",
       "      '/Q',\n",
       "      '/R',\n",
       "      '/S',\n",
       "      '/T',\n",
       "      '/U',\n",
       "      '/V',\n",
       "      '/W',\n",
       "      '/X',\n",
       "      '/Y',\n",
       "      '/Z',\n",
       "      97,\n",
       "      '/a',\n",
       "      '/b',\n",
       "      '/c',\n",
       "      '/d',\n",
       "      '/e',\n",
       "      '/f',\n",
       "      '/g',\n",
       "      '/h',\n",
       "      '/i',\n",
       "      '/j',\n",
       "      '/k',\n",
       "      '/l',\n",
       "      '/m',\n",
       "      '/n',\n",
       "      '/o',\n",
       "      '/p',\n",
       "      '/q',\n",
       "      '/r',\n",
       "      '/s',\n",
       "      '/t',\n",
       "      '/u',\n",
       "      '/v',\n",
       "      '/w',\n",
       "      '/x',\n",
       "      '/y',\n",
       "      '/z',\n",
       "      134,\n",
       "      '/dagger',\n",
       "      '/daggerdbl',\n",
       "      147,\n",
       "      '/quotedblleft',\n",
       "      '/quotedblright',\n",
       "      150,\n",
       "      '/endash',\n",
       "      '/emdash',\n",
       "      201,\n",
       "      '/Eacute',\n",
       "      228,\n",
       "      '/adieresis',\n",
       "      239,\n",
       "      '/idieresis',\n",
       "      252,\n",
       "      '/udieresis'],\n",
       "     '/Type': '/Encoding'},\n",
       "    '/FirstChar': 2,\n",
       "    '/FontDescriptor': {'/Ascent': 669,\n",
       "     '/CapHeight': 669,\n",
       "     '/Descent': -193,\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-169, -270, 1010, 924],\n",
       "     '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "     '/FontName': '/DJQNQL+NimbusRomNo9L-ReguItal',\n",
       "     '/ItalicAngle': -15,\n",
       "     '/StemV': 78,\n",
       "     '/Type': '/FontDescriptor',\n",
       "     '/XHeight': 441},\n",
       "    '/LastChar': 124,\n",
       "    '/Subtype': '/Type1',\n",
       "    '/Type': '/Font',\n",
       "    '/Widths': [500,\n",
       "     500,\n",
       "     167,\n",
       "     333,\n",
       "     556,\n",
       "     278,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     333,\n",
       "     675,\n",
       "     0,\n",
       "     556,\n",
       "     389,\n",
       "     333,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     214,\n",
       "     250,\n",
       "     333,\n",
       "     420,\n",
       "     500,\n",
       "     500,\n",
       "     833,\n",
       "     778,\n",
       "     333,\n",
       "     333,\n",
       "     333,\n",
       "     500,\n",
       "     675,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     333,\n",
       "     675,\n",
       "     675,\n",
       "     675,\n",
       "     500,\n",
       "     920,\n",
       "     611,\n",
       "     611,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     333,\n",
       "     444,\n",
       "     667,\n",
       "     556,\n",
       "     833,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     722,\n",
       "     611,\n",
       "     500,\n",
       "     556,\n",
       "     722,\n",
       "     611,\n",
       "     833,\n",
       "     611,\n",
       "     556,\n",
       "     556,\n",
       "     389,\n",
       "     278,\n",
       "     389,\n",
       "     422,\n",
       "     500,\n",
       "     333,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     444,\n",
       "     278,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     389,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     444,\n",
       "     667,\n",
       "     444,\n",
       "     444,\n",
       "     389,\n",
       "     0,\n",
       "     0]}},\n",
       "  '/ProcSet': ['/PDF', '/Text'],\n",
       "  '/XObject': {'/Fm0': {'/BBox': [0, 0, 612, 792],\n",
       "    '/Filter': '/FlateDecode',\n",
       "    '/Resources': {'/Font': {'/T1_0': {'/BaseFont': '/TJXRBB+MinionPro-Regular3',\n",
       "       '/Encoding': {'/Differences': [16, '/r', 86, '/t', '/T'],\n",
       "        '/Type': '/Encoding'},\n",
       "       '/FirstChar': 16,\n",
       "       '/FontDescriptor': {'/Ascent': 631,\n",
       "        '/CapHeight': 662,\n",
       "        '/Descent': -10,\n",
       "        '/Flags': 4,\n",
       "        '/FontBBox': [-290, -360, 1684, 989],\n",
       "        '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "        '/FontName': '/TJXRBB+MinionPro-Regular3',\n",
       "        '/ItalicAngle': 0,\n",
       "        '/StemV': 79,\n",
       "        '/Type': '/FontDescriptor',\n",
       "        '/XHeight': 438},\n",
       "       '/LastChar': 88,\n",
       "       '/Subtype': '/Type1',\n",
       "       '/Type': '/Font',\n",
       "       '/Widths': [371,\n",
       "        621,\n",
       "        371,\n",
       "        621,\n",
       "        528,\n",
       "        0,\n",
       "        561,\n",
       "        371,\n",
       "        621,\n",
       "        528,\n",
       "        0,\n",
       "        321,\n",
       "        501,\n",
       "        563,\n",
       "        400,\n",
       "        400,\n",
       "        367,\n",
       "        528,\n",
       "        0,\n",
       "        367,\n",
       "        474,\n",
       "        367,\n",
       "        474,\n",
       "        408,\n",
       "        0,\n",
       "        367,\n",
       "        474,\n",
       "        408,\n",
       "        408,\n",
       "        0,\n",
       "        366,\n",
       "        477,\n",
       "        412,\n",
       "        0,\n",
       "        408,\n",
       "        0,\n",
       "        477,\n",
       "        228,\n",
       "        480,\n",
       "        341,\n",
       "        464,\n",
       "        341,\n",
       "        341,\n",
       "        437,\n",
       "        341,\n",
       "        486,\n",
       "        515,\n",
       "        598,\n",
       "        480,\n",
       "        341,\n",
       "        495,\n",
       "        341,\n",
       "        341,\n",
       "        499,\n",
       "        341,\n",
       "        486,\n",
       "        331,\n",
       "        227,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        408,\n",
       "        0,\n",
       "        480,\n",
       "        482,\n",
       "        481,\n",
       "        486,\n",
       "        536,\n",
       "        305,\n",
       "        617,\n",
       "        0]},\n",
       "      '/T1_1': {'/BaseFont': '/TJXRBB+MinionPro-Regular',\n",
       "       '/Encoding': {'/Differences': [23,\n",
       "         '/g',\n",
       "         42,\n",
       "         '/h',\n",
       "         50,\n",
       "         '/hyphen',\n",
       "         52,\n",
       "         '/i',\n",
       "         '/I',\n",
       "         100,\n",
       "         '/l',\n",
       "         134,\n",
       "         '/n',\n",
       "         164,\n",
       "         '/o',\n",
       "         226,\n",
       "         '/parenleft'],\n",
       "        '/Type': '/Encoding'},\n",
       "       '/FirstChar': 23,\n",
       "       '/FontDescriptor': {'/Ascent': 631,\n",
       "        '/CapHeight': 662,\n",
       "        '/Descent': -10,\n",
       "        '/Flags': 4,\n",
       "        '/FontBBox': [-290, -360, 1684, 989],\n",
       "        '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "        '/FontName': '/TJXRBB+MinionPro-Regular',\n",
       "        '/ItalicAngle': 0,\n",
       "        '/StemV': 79,\n",
       "        '/Type': '/FontDescriptor',\n",
       "        '/XHeight': 438},\n",
       "       '/LastChar': 227,\n",
       "       '/Subtype': '/Type1',\n",
       "       '/Type': '/Font',\n",
       "       '/Widths': [468,\n",
       "        715,\n",
       "        495,\n",
       "        507,\n",
       "        468,\n",
       "        715,\n",
       "        565,\n",
       "        0,\n",
       "        545,\n",
       "        400,\n",
       "        400,\n",
       "        401,\n",
       "        552,\n",
       "        565,\n",
       "        0,\n",
       "        444,\n",
       "        445,\n",
       "        279,\n",
       "        279,\n",
       "        534,\n",
       "        766,\n",
       "        533,\n",
       "        621,\n",
       "        0,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        356,\n",
       "        259,\n",
       "        268,\n",
       "        341,\n",
       "        268,\n",
       "        341,\n",
       "        306,\n",
       "        0,\n",
       "        268,\n",
       "        341,\n",
       "        306,\n",
       "        0,\n",
       "        268,\n",
       "        341,\n",
       "        306,\n",
       "        0,\n",
       "        341,\n",
       "        0,\n",
       "        268,\n",
       "        341,\n",
       "        306,\n",
       "        0,\n",
       "        522,\n",
       "        669,\n",
       "        603,\n",
       "        0,\n",
       "        760,\n",
       "        375,\n",
       "        245,\n",
       "        341,\n",
       "        245,\n",
       "        341,\n",
       "        245,\n",
       "        245,\n",
       "        306,\n",
       "        0,\n",
       "        268,\n",
       "        341,\n",
       "        306,\n",
       "        0,\n",
       "        256,\n",
       "        329,\n",
       "        297,\n",
       "        0,\n",
       "        496,\n",
       "        673,\n",
       "        493,\n",
       "        673,\n",
       "        558,\n",
       "        0,\n",
       "        253,\n",
       "        538,\n",
       "        253,\n",
       "        538,\n",
       "        460,\n",
       "        0,\n",
       "        493,\n",
       "        686,\n",
       "        253,\n",
       "        538,\n",
       "        460,\n",
       "        0,\n",
       "        552,\n",
       "        480,\n",
       "        486,\n",
       "        481,\n",
       "        486,\n",
       "        580,\n",
       "        460,\n",
       "        273,\n",
       "        550,\n",
       "        478,\n",
       "        0,\n",
       "        819,\n",
       "        891,\n",
       "        400,\n",
       "        400,\n",
       "        401,\n",
       "        580,\n",
       "        709,\n",
       "        0,\n",
       "        512,\n",
       "        891,\n",
       "        580,\n",
       "        547,\n",
       "        743,\n",
       "        547,\n",
       "        743,\n",
       "        580,\n",
       "        0,\n",
       "        547,\n",
       "        743,\n",
       "        580,\n",
       "        0,\n",
       "        480,\n",
       "        341,\n",
       "        488,\n",
       "        341,\n",
       "        341,\n",
       "        486,\n",
       "        341,\n",
       "        486,\n",
       "        580,\n",
       "        0,\n",
       "        547,\n",
       "        743,\n",
       "        580,\n",
       "        0,\n",
       "        476,\n",
       "        743,\n",
       "        480,\n",
       "        482,\n",
       "        519,\n",
       "        486,\n",
       "        510,\n",
       "        747,\n",
       "        510,\n",
       "        747,\n",
       "        584,\n",
       "        0,\n",
       "        510,\n",
       "        747,\n",
       "        584,\n",
       "        0,\n",
       "        510,\n",
       "        747,\n",
       "        584,\n",
       "        0,\n",
       "        770,\n",
       "        973,\n",
       "        776,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        510,\n",
       "        747,\n",
       "        584,\n",
       "        0,\n",
       "        525,\n",
       "        747,\n",
       "        0,\n",
       "        510,\n",
       "        747,\n",
       "        584,\n",
       "        0,\n",
       "        645,\n",
       "        757,\n",
       "        645,\n",
       "        510,\n",
       "        747,\n",
       "        510,\n",
       "        480,\n",
       "        341,\n",
       "        338,\n",
       "        762,\n",
       "        341,\n",
       "        341,\n",
       "        340,\n",
       "        762,\n",
       "        341,\n",
       "        486,\n",
       "        305,\n",
       "        334,\n",
       "        584,\n",
       "        513,\n",
       "        749,\n",
       "        582,\n",
       "        0,\n",
       "        0,\n",
       "        510,\n",
       "        747,\n",
       "        584,\n",
       "        0,\n",
       "        524,\n",
       "        563,\n",
       "        497,\n",
       "        346,\n",
       "        0]},\n",
       "      '/T1_2': {'/BaseFont': '/TJXRBB+MinionPro-Regular2',\n",
       "       '/Encoding': {'/Differences': [0,\n",
       "         '/a',\n",
       "         '/A',\n",
       "         97,\n",
       "         '/c',\n",
       "         '/C',\n",
       "         175,\n",
       "         '/e',\n",
       "         239,\n",
       "         '/f',\n",
       "         '/F'],\n",
       "        '/Type': '/Encoding'},\n",
       "       '/FirstChar': 0,\n",
       "       '/FontDescriptor': {'/Ascent': 631,\n",
       "        '/CapHeight': 662,\n",
       "        '/Descent': -10,\n",
       "        '/Flags': 4,\n",
       "        '/FontBBox': [-290, -360, 1684, 989],\n",
       "        '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "        '/FontName': '/TJXRBB+MinionPro-Regular2',\n",
       "        '/ItalicAngle': 0,\n",
       "        '/StemV': 79,\n",
       "        '/Type': '/FontDescriptor',\n",
       "        '/XHeight': 438},\n",
       "       '/LastChar': 241,\n",
       "       '/Subtype': '/Type1',\n",
       "       '/Type': '/Font',\n",
       "       '/Widths': [439,\n",
       "        691,\n",
       "        439,\n",
       "        691,\n",
       "        565,\n",
       "        0,\n",
       "        439,\n",
       "        691,\n",
       "        565,\n",
       "        0,\n",
       "        439,\n",
       "        691,\n",
       "        565,\n",
       "        0,\n",
       "        400,\n",
       "        400,\n",
       "        401,\n",
       "        439,\n",
       "        691,\n",
       "        565,\n",
       "        0,\n",
       "        671,\n",
       "        869,\n",
       "        723,\n",
       "        439,\n",
       "        691,\n",
       "        565,\n",
       "        0,\n",
       "        523,\n",
       "        691,\n",
       "        523,\n",
       "        711,\n",
       "        622,\n",
       "        230,\n",
       "        439,\n",
       "        691,\n",
       "        568,\n",
       "        0,\n",
       "        439,\n",
       "        691,\n",
       "        565,\n",
       "        0,\n",
       "        565,\n",
       "        566,\n",
       "        580,\n",
       "        404,\n",
       "        0,\n",
       "        753,\n",
       "        439,\n",
       "        691,\n",
       "        565,\n",
       "        0,\n",
       "        508,\n",
       "        588,\n",
       "        333,\n",
       "        263,\n",
       "        510,\n",
       "        588,\n",
       "        347,\n",
       "        347,\n",
       "        345,\n",
       "        345,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        402,\n",
       "        263,\n",
       "        511,\n",
       "        0,\n",
       "        390,\n",
       "        974,\n",
       "        1124,\n",
       "        1133,\n",
       "        957,\n",
       "        457,\n",
       "        603,\n",
       "        623,\n",
       "        830,\n",
       "        1006,\n",
       "        806,\n",
       "        1408,\n",
       "        1744,\n",
       "        1095,\n",
       "        643,\n",
       "        566,\n",
       "        821,\n",
       "        836,\n",
       "        906,\n",
       "        1602,\n",
       "        1675,\n",
       "        1584,\n",
       "        427,\n",
       "        892,\n",
       "        745,\n",
       "        745,\n",
       "        465,\n",
       "        619,\n",
       "        423,\n",
       "        665,\n",
       "        423,\n",
       "        665,\n",
       "        531,\n",
       "        0,\n",
       "        400,\n",
       "        400,\n",
       "        401,\n",
       "        423,\n",
       "        665,\n",
       "        531,\n",
       "        0,\n",
       "        421,\n",
       "        661,\n",
       "        528,\n",
       "        0,\n",
       "        400,\n",
       "        400,\n",
       "        381,\n",
       "        480,\n",
       "        479,\n",
       "        468,\n",
       "        486,\n",
       "        467,\n",
       "        654,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        401,\n",
       "        228,\n",
       "        480,\n",
       "        568,\n",
       "        528,\n",
       "        486,\n",
       "        228,\n",
       "        177,\n",
       "        177,\n",
       "        177,\n",
       "        702,\n",
       "        531,\n",
       "        0,\n",
       "        480,\n",
       "        528,\n",
       "        735,\n",
       "        490,\n",
       "        489,\n",
       "        528,\n",
       "        735,\n",
       "        584,\n",
       "        0,\n",
       "        526,\n",
       "        737,\n",
       "        611,\n",
       "        0,\n",
       "        343,\n",
       "        497,\n",
       "        641,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        404,\n",
       "        400,\n",
       "        580,\n",
       "        480,\n",
       "        480,\n",
       "        473,\n",
       "        486,\n",
       "        480,\n",
       "        400,\n",
       "        400,\n",
       "        400,\n",
       "        268,\n",
       "        306,\n",
       "        584,\n",
       "        0,\n",
       "        425,\n",
       "        568,\n",
       "        425,\n",
       "        568,\n",
       "        482,\n",
       "        0,\n",
       "        425,\n",
       "        568,\n",
       "        482,\n",
       "        0,\n",
       "        425,\n",
       "        568,\n",
       "        482,\n",
       "        0,\n",
       "        425,\n",
       "        568,\n",
       "        482,\n",
       "        0,\n",
       "        425,\n",
       "        568,\n",
       "        482,\n",
       "        0,\n",
       "        480,\n",
       "        341,\n",
       "        475,\n",
       "        341,\n",
       "        341,\n",
       "        466,\n",
       "        341,\n",
       "        486,\n",
       "        970,\n",
       "        922,\n",
       "        520,\n",
       "        541,\n",
       "        736,\n",
       "        579,\n",
       "        0,\n",
       "        424,\n",
       "        568,\n",
       "        487,\n",
       "        0,\n",
       "        403,\n",
       "        568,\n",
       "        403,\n",
       "        580,\n",
       "        482,\n",
       "        790,\n",
       "        711,\n",
       "        0,\n",
       "        509,\n",
       "        766,\n",
       "        509,\n",
       "        505,\n",
       "        737,\n",
       "        611,\n",
       "        0,\n",
       "        480,\n",
       "        574,\n",
       "        529,\n",
       "        486,\n",
       "        276,\n",
       "        276,\n",
       "        235,\n",
       "        242,\n",
       "        296,\n",
       "        529,\n",
       "        0]},\n",
       "      '/T1_3': {'/BaseFont': '/TJXRBB+MinionPro-Regular4',\n",
       "       '/Encoding': {'/Differences': [133, '/y'], '/Type': '/Encoding'},\n",
       "       '/FirstChar': 133,\n",
       "       '/FontDescriptor': {'/Ascent': 631,\n",
       "        '/CapHeight': 662,\n",
       "        '/Descent': -10,\n",
       "        '/Flags': 4,\n",
       "        '/FontBBox': [-290, -360, 1684, 989],\n",
       "        '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "        '/FontName': '/TJXRBB+MinionPro-Regular4',\n",
       "        '/ItalicAngle': 0,\n",
       "        '/StemV': 79,\n",
       "        '/Type': '/FontDescriptor',\n",
       "        '/XHeight': 438},\n",
       "       '/LastChar': 134,\n",
       "       '/Subtype': '/Type1',\n",
       "       '/Type': '/Font',\n",
       "       '/Widths': [459, 0]},\n",
       "      '/T1_4': {'/BaseFont': '/VSAAEP+TeXGyreTermes-Regular',\n",
       "       '/Encoding': {'/Differences': [41, '/parenright', 49, '/one', '/two'],\n",
       "        '/Type': '/Encoding'},\n",
       "       '/FirstChar': 41,\n",
       "       '/FontDescriptor': {'/Ascent': 672,\n",
       "        '/CapHeight': 672,\n",
       "        '/Descent': -218,\n",
       "        '/Flags': 4,\n",
       "        '/FontBBox': [-526, -281, 1306, 1055],\n",
       "        '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "        '/FontName': '/VSAAEP+TeXGyreTermes-Regular',\n",
       "        '/ItalicAngle': 0,\n",
       "        '/StemV': 102,\n",
       "        '/Type': '/FontDescriptor',\n",
       "        '/XHeight': 450},\n",
       "       '/LastChar': 51,\n",
       "       '/Subtype': '/Type1',\n",
       "       '/Type': '/Font',\n",
       "       '/Widths': [333, 500, 564, 250, 333, 250, 278, 500, 500, 500, 0]},\n",
       "      '/T1_5': {'/BaseFont': '/IYUAMN+Utopia-Regular',\n",
       "       '/Encoding': {'/Differences': [48,\n",
       "         '/zero',\n",
       "         50,\n",
       "         '/two',\n",
       "         '/three',\n",
       "         '/four',\n",
       "         '/five',\n",
       "         '/six',\n",
       "         '/seven',\n",
       "         '/eight',\n",
       "         '/nine'],\n",
       "        '/Type': '/Encoding'},\n",
       "       '/FirstChar': 48,\n",
       "       '/FontDescriptor': {'/Ascent': 737,\n",
       "        '/CapHeight': 705,\n",
       "        '/Descent': -233,\n",
       "        '/Flags': 4,\n",
       "        '/FontBBox': [-158, -250, 1158, 890],\n",
       "        '/FontFile3': {'/Filter': '/FlateDecode', '/Subtype': '/Type1C'},\n",
       "        '/FontName': '/IYUAMN+Utopia-Regular',\n",
       "        '/ItalicAngle': 0,\n",
       "        '/StemV': 94,\n",
       "        '/Type': '/FontDescriptor',\n",
       "        '/XHeight': 490},\n",
       "       '/LastChar': 58,\n",
       "       '/Subtype': '/Type1',\n",
       "       '/Type': '/Font',\n",
       "       '/Widths': [530, 530, 530, 530, 530, 530, 530, 530, 530, 530, 0]}},\n",
       "     '/ProcSet': ['/PDF', '/Text']},\n",
       "    '/Subtype': '/Form',\n",
       "    '/Type': '/XObject'}}},\n",
       " '/Rotate': 0,\n",
       " '/Type': '/Page',\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Count': 1,\n",
       "  '/Kids': [IndirectObject(4, 0, 2312885861632)]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:45:34.375709Z",
     "start_time": "2024-05-30T05:45:34.367674Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_output = open('MY_BRAND_NEW_PDF.pdf','wb')",
   "id": "5cb99656a11af1cb",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:53:18.457903Z",
     "start_time": "2024-05-30T05:53:18.436884Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_writer.write(pdf_output)",
   "id": "21863d26b615872",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, <_io.BufferedWriter name='MY_BRAND_NEW_PDF.pdf'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:53:41.756572Z",
     "start_time": "2024-05-30T05:53:41.687091Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_output.close()",
   "id": "b70c3729a3621d1d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:53:44.317785Z",
     "start_time": "2024-05-30T05:53:44.302770Z"
    }
   },
   "cell_type": "code",
   "source": "f.close()",
   "id": "7caeb239daf66059",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:54:15.113554Z",
     "start_time": "2024-05-30T05:54:15.093532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "brand_new = open('MY_BRAND_NEW_PDF.pdf','rb')\n",
    "pdf_reader= PyPDF2.PdfReader(brand_new)"
   ],
   "id": "dc0ba04d3c50faec",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:54:47.069488Z",
     "start_time": "2024-05-30T05:54:47.058478Z"
    }
   },
   "cell_type": "code",
   "source": "len(pdf_reader.pages)",
   "id": "92aa0c9c4c6b4f72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:59:35.037644Z",
     "start_time": "2024-05-30T05:59:34.705337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f = open('meta-learning.pdf','rb')\n",
    "pdf_text = []\n",
    "pdf_reader = PyPDF2.PdfReader(f)\n",
    "for p in range(len(pdf_reader.pages)):\n",
    "    page=pdf_reader.pages[p]\n",
    "    pdf_text.append(page.extract_text())\n",
    "f.close()"
   ],
   "id": "99923c47aaaee1a5",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T05:59:39.381820Z",
     "start_time": "2024-05-30T05:59:39.365813Z"
    }
   },
   "cell_type": "code",
   "source": "pdf_text",
   "id": "34b34d4ebe80250c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meta-Learning Framework with Applications to Zero-Shot Time-Series\\nForecasting\\nBoris N. Oreshkin1, Dmitri Carpov1, Nicolas Chapados1, Yoshua Bengio2\\n1Element AI,2Mila\\nboris.oreshkin@gmail.com\\nAbstract\\nCan meta-learning discover generic ways of processing time\\nseries (TS) from a diverse dataset so as to greatly improve\\ngeneralization on new TS coming from different datasets?\\nThis work provides positive evidence to this using a broad\\nmeta-learning framework which we show subsumes many\\nexisting meta-learning algorithms. Our theoretical analysis\\nsuggests that residual connections act as a meta-learning adap-\\ntation mechanism, generating a subset of task-speciﬁc param-\\neters based on a given TS input, thus gradually expanding\\nthe expressive power of the architecture on-the-ﬂy. The same\\nmechanism is shown via linearization analysis to have the\\ninterpretation of a sequential update of the ﬁnal linear layer.\\nOur empirical results on a wide range of data emphasize the\\nimportance of the identiﬁed meta-learning mechanisms for\\nsuccessful zero-shot univariate forecasting, suggesting that it\\nis viable to train a neural network on a source TS dataset and\\ndeploy it on a different target TS dataset without retraining,\\nresulting in performance that is at least as good as that of\\nstate-of-practice univariate forecasting models.\\n1 Introduction\\nTime series (TS) forecasting is both a fundamental scientiﬁc\\nproblem and one of great practical importance. It is central\\nto the actions of intelligent agents: the ability to plan and\\ncontrol as well as to appropriately react to manifestations\\nof complex partially or completely unknown systems often\\nrelies on the ability to forecast relevant observations based on\\npast history. Moreover, for most utility-maximizing agents,\\ngains in forecasting accuracy broadly translate into utility\\ngains; as such, improvements in forecasting technology can\\nhave wide impacts. Unsurprisingly, forecasting methods have\\na long history that can be traced back to the very origins\\nof human civilization (Neale 1985), modern science (Gauss\\n1809) and have consistently attracted considerable research\\nattention (Yule 1927; Walker 1931; Holt 1957; Winters 1960;\\nEngle 1982; Sezer, Gudelek, and Ozbayoglu 2019). The appli-\\ncations of forecasting span a variety of ﬁelds, including high-\\nfrequency control (e.g. vehicle and robot control (Tang and\\nSalakhutdinov 2019), data center optimization (Gao 2014)),\\nbusiness planning (supply chain management (Leung 1995),\\nworkforce and call center management (Chapados et al. 2014;\\nCopyright c\\r2021, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.Ibrahim et al. 2016), as well as such critically important areas\\nas precision agriculture (Rodrigues Jr et al. 2019). In business\\nspeciﬁcally, improved forecasting translates in better pro-\\nduction planning (leading to less waste) and less transporta-\\ntion (reducing CO 2emissions) (Kahn 2003; Kerkkänen, Kor-\\npela, and Huiskonen 2009; Nguyen, Ni, and Rossetti 2010).\\nThe progress made in univariate forecasting in the past four\\ndecades is well reﬂected in the results and methods consid-\\nered in associated competitions over that period (Makridakis\\net al. 1982, 1993; Makridakis and Hibon 2000; Athanasopou-\\nlos et al. 2011; Makridakis, Spiliotis, and Assimakopoulos\\n2018a). Recently, growing evidence has started to emerge\\nsuggesting that machine learning approaches could improve\\non classical forecasting methods, in contrast to some ear-\\nlier assessments (Makridakis, Spiliotis, and Assimakopoulos\\n2018b). For example, the winner of the 2018 M4 competition\\n(Makridakis, Spiliotis, and Assimakopoulos 2018a) was a\\nneural network designed by Smyl (2020).\\nOn the practical side, the deployment of deep neural time-\\nseries models is challenged by the cold start problem. Before\\natabula rasa deep neural network provides a useful forecast-\\ning output, it should be trained on a large problem-speciﬁc\\ntime-series dataset. For early adopters, this often implies data\\ncollection efforts, changing data handling practices and even\\nchanging the existing IT infrastructures on a large scale. In\\ncontrast, advanced statistical models can be deployed with\\nsigniﬁcantly less effort as they estimate their parameters on\\nsingle time series at a time. In this paper we address the prob-\\nlem of reducing the entry cost of deep neural networks in\\nthe industrial practice of TS forecasting. We show that it is\\nviable to train a neural network model on a diversiﬁed source\\ndataset and deploy it on a target dataset in a zero-shot regime,\\ni.e. without explicit retraining on that target data, resulting\\nin performance that is at least as good as that of advanced\\nstatistical models tailored to the target dataset. We would like\\nto clarify that we use the term “zero-shot” in our work in the\\nsense that the number of history samples available for the\\ntarget time series is so small that it makes training a deep\\nlearning model on this time series infeasible.\\nAddressing this practical problem provides clues to fun-\\ndamental questions. Can we learn something general about\\nforecasting and transfer this knowledge across datasets? If\\nso, what kind of mechanisms could facilitate this? The abil-\\nity to learn and transfer representations across tasks via\\nTheThirty-FifthAAAIConferenceonArtificial Intelligence(AAAI-21)\\n9242',\n",
       " 'task adaptation is an advantage of meta-learning (Raghu\\net al. 2020). We propose here a broad theoretical framework\\nfor meta-learning that encompasses several existing meta-\\nlearning algorithms. We further show that a recent successful\\nmodel, N-BEATS (Oreshkin et al. 2020), ﬁts this framework.\\nWe identify internal meta-learning adaptation mechanisms\\nthat generate new parameters on-the-ﬂy, speciﬁc to a given\\nTS, iteratively extending the architecture’s expressive power.\\nWe empirically conﬁrm that meta-learning mechanisms are\\nkey to improving zero-shot TS forecasting performance, and\\ndemonstrate results on a wide range of datasets.\\n1.1 Background\\nThe univariate point forecasting problem in discrete time\\nis formulated given a length- Hforecast horizon and a\\nlength- Tobserved series history [y1;:::; yT]2RT. The\\ntask is to predict the vector of future values y2RH=\\n[yT+1;yT+2;:::; yT+H]. For simplicity, we will later consider\\nalookback window of length t\\x14Tending with the last\\nobserved value yTto serve as model input, and denoted\\nx2Rt= [yT\\x00t+1;:::; yT]. We denotebythe point forecast\\nofy. Its accuracy can be evaluated with sMAPE , the sym-\\nmetric mean absolute percentage error (Makridakis, Spiliotis,\\nand Assimakopoulos 2018a),\\nsMAPE =200\\nHH\\nå\\ni=1jyT+i\\x00byT+ij\\njyT+ij+jbyT+ij: (1)\\nOther quality metrics (e.g. MAPE ,MASE ,OWA ,ND) are pos-\\nsible and are deﬁned in Appendix A.\\nMeta-learning orlearning-to-learn (Harlow 1949; Schmi-\\ndhuber 1987; Bengio, Bengio, and Cloutier 1991) is usually\\nlinked to being able to (i) accumulate knowledge across tasks\\n(i.e. transfer learning, multi-task learning) and (ii) quickly\\nadapt the accumulated knowledge to the new task (task adap-\\ntation) (Ravi and Larochelle 2016; Bengio et al. 1992).\\nN-BEATS algorithm has demonstrated outstanding perfor-\\nmance on several competition benchmarks (Oreshkin et al.\\n2020). The model consists of a total of Lblocks connected\\nusing a doubly residual architecture. Block `has input x`\\nand produces two outputs: the backcastbx`and the partial\\nforecastby`. For the ﬁrst block we deﬁne x1\\x11x, where x\\nis assumed to be the model-level input from now on. We\\ndeﬁne the k-th fully-connected layer in the `-th block; having\\nRELUnon-linearity, weights Wk, bias bkand input h`;k\\x001, as\\nFCk(h`;k\\x001)\\x11RELU(W kh`;k\\x001+bk). We focus on the con-\\nﬁguration that shares all learnable parameters across blocks.\\nWith this notation, one block of N-BEATS is described as:\\nh`;1=FC1(x`);h`;k=FCk(h`;k\\x001);k=2:::K;\\nbx`=Qh`;K;by`=Gh`;K;(2)\\nwhere QandGare linear operators. The N-BEATS parame-\\nters included in the FCand linear layers are learned by min-\\nimizing a suitable loss function (e.g. sMAPE deﬁned in (1))\\nacross multiple TS. Finally, the doubly residual architecture is\\ndescribed by the following recursion (recalling that x1\\x11x):\\nx`=x`\\x001\\x00bx`\\x001;by=L\\nå\\n`=1by`: (3)1.2 Related Work\\nFrom a high-level perspective, there are many links with\\nclassical TS modeling: a human-speciﬁed classical model is\\ntypically designed to generalize well on unseen TS, while\\nwe propose to automate that process. The classical models\\ninclude exponential smoothing with and without seasonal\\neffects (Holt 1957, 2004; Winters 1960), multi-trace expo-\\nnential smoothing approaches, e.g.Theta and its variants (As-\\nsimakopoulos and Nikolopoulos 2000; Fiorucci et al. 2016;\\nSpiliotis, Assimakopoulos, and Nikolopoulos 2019). Finally,\\nthe state space modeling approach encapsulates most of the\\nabove in addition to auto-ARIMA and GARCH (Engle 1982;\\nsee Hyndman and Khandakar (2008) for an overview). The\\nstate-space approach has also been underlying signiﬁcant\\namounts of research in the neural TS modeling (Salinas et al.\\n2019; Wang et al. 2019; Rangapuram et al. 2018). However,\\nthose models have not been considered in the zero-shot sce-\\nnario. In this work we focus on studying the importance\\nof meta-learning for successful zero-shot forecasting. The\\nfoundations of meta-learning have been developed by Schmi-\\ndhuber (1987); Bengio, Bengio, and Cloutier (1991) among\\nothers. More recently, meta-learning research has been ex-\\npanding, mostly outside of the TS forecasting domain (Ravi\\nand Larochelle 2016; Finn, Abbeel, and Levine 2017; Snell,\\nSwersky, and Zemel 2017; Vinyals et al. 2016; Rusu et al.\\n2019). In the TS domain, meta-learning has manifested it-\\nself via neural models trained over a collection of TS (Smyl\\n2020; Oreshkin et al. 2020) or via a model trained to predict\\nweights combining outputs of several classical forecasting\\nalgorithms (Montero-Manso et al. 2020). Successful appli-\\ncation of a neural TS forecasting model trained on a source\\ndataset and ﬁne-tuned on the target dataset was demonstrated\\nby Hooshmand and Sharma (2019); Ribeiro et al. (2018) as\\nwell as in the context of TS classiﬁcation by Fawaz et al.\\n(2018). Unlike those, we focus on the zero-shot scenario and\\naddress the cold start problem.\\n1.3 Summary of Contributions\\nWe deﬁne a meta-learning framework with associated equa-\\ntions, and recast within it many existing meta-learning algo-\\nrithms. We show that N-BEATS follows the same equations.\\nAccording to our analysis, its residual connections implement\\nmeta-learning inner loop, thereby performing task adaptation\\nwithout gradient steps at inference time.\\nWe deﬁne a novel zero-shot univariate TS forecasting\\ntask and make its dataset loaders and evaluation code public,\\nincluding a new large-scale dataset ( FRED ) with 290k TS.\\nWe empirically show, for the ﬁrst time, that deep-learning\\nzero-shot time series forecasting is feasible and that the\\nmeta-learning component is important for zero-shot general-\\nization in univariate TS forecasting.\\n2 Meta-Learning Framework\\nA meta-learning procedure can generally be viewed at two\\nlevels: the inner loop and the outer loop. The inner training\\nloop operates within an individual “meta-example” or task T\\n(fast learning loop improving over current T) and the outer\\nloop operates across tasks (slow learning loop). A task T\\n9243',\n",
       " 'includes task training data Dtr\\nTand task validation data Dval\\nT,\\nboth optionally involving inputs, targets and a task-speciﬁc\\nloss:Dtr\\nT=fXtr\\nT;Ytr\\nT;LTg,Dval\\nT=fXval\\nT;Yval\\nT;LTg. Accord-\\ningly, a meta-learning set-up can be deﬁned by assuming a\\ndistribution p(T)over tasks, a predictor Pq;wand a meta-\\nlearner with meta-parameters j. We allow a subset of predic-\\ntor’s parameters denoted wto belong to meta-parameters j\\nand hence not to be task adaptive. The objective is to design\\na meta-learner that can generalize well on a new task by ap-\\npropriately choosing the predictor’s task adaptive parameters\\nqafter observing Dtr\\nT. The meta-learner is trained to do so\\nby being exposed to many tasks in a training dataset fTtrain\\nig\\nsampled from p(T). For each training task Ttrain\\ni, the meta-\\nlearner is requested to produce the solution to the task in the\\nform of Pq;w:Xval\\nTi7!bYval\\nTiconditioned on Dtr\\nTi. The meta-\\nparameters jare updated in the outer meta-learning loop so\\nas to obtain good generalization in the inner loop, i.e., by\\nminimizing the expected validation loss ETiLTi(bYval\\nTi;Yval\\nTi)\\nmapping the ground truth and estimated outputs into the value\\nthat quantiﬁes the generalization performance across tasks.\\nTraining on multiple tasks enables the meta-learner to pro-\\nduce solutions Pq;wthat generalize well on a set of unseen\\ntasksfTtest\\nigsampled from p(T).\\nConsequently, the meta-learning procedure has three\\ndistinct ingredients: (i) meta-parameters j= (t0;w;u),\\n(ii) initialization function It0and (iii) update function Uu.\\nThe meta-learner’s meta-parameters jinclude the meta-\\nparameters of the meta-initialization function, t0, the meta-\\nparameters of the predictor shared across tasks, w, and\\nthe meta-parameters of the update function, u. The meta-\\ninitialization function It0(Dtr\\nTi;cTi)deﬁnes the initial val-\\nues of parameters qfor a given task Tibased on its meta-\\ninitialization parameters t0, task training dataset Dtr\\nTiand\\ntask meta-data cTi. Task meta-data may have, for example,\\na form of task ID or a textual task description. The update\\nfunction Uu(q`\\x001;Dtr\\nTi)is parameterized with update meta-\\nparameters u. It deﬁnes an iterated update to predictor pa-\\nrameters qat iteration `based on their previous value and\\nthe task training set Dtr\\nTi. The initialization and update func-\\ntions produce a sequence of predictor parameters, which we\\ncompactly write as q0:`\\x11fq0;:::;q`\\x001;q`g. We let the ﬁnal\\npredictor be a function of the whole sequence of parame-\\nters, written compactly as Pq0:`;w. One implementation of\\nsuch general function could be a Bayesian ensemble or a\\nweighted sum, for example: Pq0:`;w(\\x01) =å`\\nj=0wjPqj;w(\\x01). If\\nwe set wj=1 iff j=`and 0 otherwise , then we get the more\\ncommon situation Pq0:`;w(\\x01)\\x11Pq`;w(\\x01). This meta-learning\\nframework is succinctly described by the following set of\\nequations:\\nParameters: q; Meta-parameters: j= (t0;w;u)\\nInner Loop: q0 It0(Dtr\\nTi;cTi)\\nq` Uu(q`\\x001;Dtr\\nTi);8` > 0(4)\\nPrediction at x:Pq0:`;w(x)\\nOuter Loop: j j\\x00hÑjLTi[Pq0:`;w(Xval\\nTi);Yval\\nTi]:(5)2.1 Meta-Learning and Time-Teries Forecasting\\nIn the previous section we laid out a unifying framework for\\nmeta-learning. How is it connected to the TS forecasting task?\\nWe believe that this question is best answered by answering\\nquestions “why the classical statistical TS forecasting mod-\\nels such as ARIMA and ETS are not doing meta-learning?”\\nand “what does the meta-learning component offer when it is\\npart of a forecasting algorithm?”. The ﬁrst question can be\\nanswered by considering the fact that the classical statistical\\nmodels produce a forecast by estimating their parameters\\nfrom the history of the target time series using a predeﬁned\\nﬁxed set of rules, for example, given a model selection and the\\nmaximum likelihood parameter estimator for it. Therefore, in\\nterms of our meta-learning framework, a classical statistical\\nmodel executes only the inner loop (model parameter estima-\\ntion) encapsulated in equation (4). The outer loop in this case\\nis irrelevant — a human analyst deﬁnes what equation (4)\\nis doing, based on experience (for example, “for most slow\\nvarying time-series with trend, no seasonality and white resid-\\nuals, ETS with Gaussian maximum likelihood estimator will\\nprobably work well”). The second question can be answered\\nconsidering that meta-learning based forecasting algorithm\\nreplaces the predeﬁned ﬁxed set of rules for model parameter\\nestimation with a learnable parameter estimation strategy.\\nThe learnable parameter estimation strategy is trained using\\nouter loop equation (5)by adjusting the strategy such that it\\nis able to produce parameter estimates that generalize well\\nover multiple TS. It is assumed that there exists a dataset that\\nis representative of the forecasting tasks that will be handled\\nat inference time. Thus the main advantage of meta-learning\\nbased forecasting approaches is that they enable learning a\\ndata-driven parameter estimator that can be optimized for\\na particular set of forecasting tasks and forecasting models.\\nOn top of that, a meta-learning approach allows for a gen-\\neral learnable predictor in equation (4)that can be optimized\\nfor a given forecasting task. So both predictor (model) and\\nits parameter estimator can be jointly learned for a forecast-\\ning task represented by available data. Empirically, we show\\nthat this elegant theoretical concept works effectively across\\nmultiple datasets and across multiple forecasting tasks (e.g.\\nforecasting yearly, monthly or hourly TS) and even across\\nvery loosely related tasks (for example, forecasting hourly\\nelectricity demand after training on a monthly economic data\\nafter appropriate time scale normalization).\\n2.2 Expressing Existing Meta-Learning\\nAlgorithms in the Proposed Framework\\nTo further illustrate the generality of the proposed framework,\\nwe next show how to cast existing meta-learning algorithms\\nwithin it, before turning to N-BEATS.\\nMAML and related approaches (Finn, Abbeel, and Levine\\n2017; Li et al. 2017; Raghu et al. 2020) can be derived\\nfrom (4)and (5)by (i) setting Ito be the identity map\\nthat copies t0intoq, (ii) setting Uto be the SGD gradient\\nupdate: Uu(q;Dtr\\nTi) =q\\x00aÑqLTi(Pq;w(Xtr\\nTi);Ytr\\nTi), where\\nu=fagand by (iii) setting the predictor’s meta-parameters\\nto the empty set w=/ 0. Equation (5)applies with no modiﬁca-\\ntions. MT-net (Lee and Choi 2018) is a variant of MAML in\\n9244',\n",
       " 'which the predictor’s meta-parameter set wis not empty. The\\npart of the predictor parameterized with wis meta-learned\\nacross tasks and is ﬁxed during task adaptation.\\nOptimization as a model for few-shot learning (Ravi\\nand Larochelle 2016) can be derived from (4)and (5)\\nvia the following steps (in addition to those of MAML).\\nFirst, set the update function Uuto the update equation\\nof an LSTM-like cell of the form ( `is the LSTM update\\nstep index) q` f`q`\\x001+a`Ñq`\\x001LTi(Pq`\\x001;w(Xtr\\nTi);Ytr\\nTi).\\nSecond, set f`to be the LSTM forget gate value (Ravi\\nand Larochelle 2016): f`=s(W F[ÑqLTi;LTi;q`\\x001;f`\\x001]+\\nbF)anda`to be the LSTM input gate value: a`=\\ns(Wa[ÑqLTi;LTi;q`\\x001;a`\\x001] +ba). Here sis a sigmoid\\nnon-linearity. Finally, include all the LSTM parameters into\\nthe set of update meta-parameters: u=fW F;bF;Wa;bag.\\nPrototypical Networks (PNs) (Snell, Swersky, and Zemel\\n2017). Most metric-based meta-learning approaches, includ-\\ning PNs, rely on comparing embeddings of the task training\\nset with those of the validation set. Therefore, it is conve-\\nnient to consider a composite predictor consisting of the\\nembedding function, Ew, and the comparison function, Cq,\\nPq;w(\\x01) =Cq\\x0eEw(\\x01). PNs can be derived from (4)and(5)\\nby considering a K-shot image classiﬁcation task, convolu-\\ntional network Ewshared across tasks and class prototypes\\npk=1\\nKåj:Ytr\\nj=kEw(Xtr\\nj)included in q=fpkg8k. Initializa-\\ntion function It0with t0=/ 0simply sets qto the values of\\nprototypes. Uuis an identity map with u=/ 0andCqis as a\\nsoftmax classiﬁer:\\nYval\\nTi=argmax\\nksoftmax(\\x00d (Ew(Xval\\nTi);pk)): (6)\\nHere d(\\x01;\\x01)is a similarity measure and the softmax is nor-\\nmalized w.r.t. all pk. Finally, deﬁne the loss LTiin(5)as\\nthe cross-entropy of the softmax classiﬁer described in (6).\\nInterestingly, q=fpkg8kare nothing else than the dynami-\\ncally generated weights of the ﬁnal linear layer fed into the\\nsoftmax, which is especially apparent when d(a;b) =\\x00a\\x01b.\\nThe fact that in the prototypical network scenario only the\\nﬁnal linear layer weights are dynamically generated based on\\nthe task training set resonates very well with the most recent\\nstudy of MAML (Raghu et al. 2020). It has been shown that\\nmost of the MAML’s gain can be recovered by only adapting\\nthe weights of the ﬁnal linear layer in the inner loop.\\nIn this section, we illustrated that four distinct meta-\\nlearning algorithms from two broad categories (optimization-\\nand metric-based) can be derived from our equations (4)and\\n(5). This conﬁrms that our meta-learning framework is gen-\\neral and it can represent existing meta-learning algorithms.\\nThe analysis of three additional existing meta-learning algo-\\nrithms is presented in Appendix C.\\n3 N-BEATS as a Meta-learning Algorithm\\nLet us now focus on the analysis of N-BEATS described by\\nequations (2),(3). We ﬁrst introduce the following notation:\\nf:x`7!h`;4;g:h`;47!by`;q:h`;47!bx`. In the original\\nequations, gandqare linear and hence can be represented\\nby equivalent matrices GandQ. In the following, we keep\\nthe notation general as much as possible, transitioning to the\\nlinear case only when needed. Then, given the network input,x(x1\\x11x), and noting that bx`\\x001=q\\x0ef(x`\\x001)we can write\\nN-BEATS as follows:\\nby=g\\x0ef(x)+å\\n`>1g\\x0ef(x`\\x001\\x00q\\x0ef(x`\\x001)): (7)\\nN-BEATS is now derived from the meta-learning framework\\nof Sec. 2 using two observations: (i) each application of g\\x0ef\\nin(7)is a predictor and (ii) each block of N-BEATS is the\\niteration of the inner meta-learning loop. More concretely,\\nwe have that Pq;w(\\x01) = gwg\\x0efwf;q(\\x01). Here wgandwfare\\nparameters of functions gand f, included in w= (wg;wf)\\nand learned across tasks in the outer loop. The task-speciﬁc\\nparameters qconsist of the sequence of input shift vec-\\ntors, q\\x11fm`gL\\n`=0, deﬁned such that the `-th block input\\ncan be written as x`=x\\x00m`\\x001. This yields a recursive ex-\\npression for the predictor’s task-speciﬁc parameters of the\\nform m` m`\\x001+bx`;m0\\x110, obtained by recursively un-\\nrolling eq. (3). These yield the following initialization and\\nupdate functions: It0with t0=/ 0setsm0to zero; Uu, with\\nu= (wq;wf)generates a next parameter update based on bx`:\\nm` Uu(m`\\x001;Dtr\\nTi)\\x11m`\\x001+qwq\\x0efwf(x\\x00m`\\x001):\\nInterestingly, (i) meta-parameters wfare shared between the\\npredictor and the update function and (ii) the task training\\nset is limited to the network input, Dtr\\nTi\\x11fxg. Note that\\nthe latter makes sense because the data are complete time\\nseries, with the inputs xhaving the same form of internal\\ndependencies as the forecasting targets y. Hence, observing\\nxis enough to infer how to predict yfrom xin a way that is\\nsimilar to how different parts of xare related to each other.\\nFinally, according to (7), predictor outputs correspond-\\ning to the values of parameters qlearned at every iter-\\nation of the inner loop are combined in the ﬁnal out-\\nput. This corresponds to choosing a predictor of the form\\nPm0:L;w(\\x01) = åL\\nj=0wjPmj;w(\\x01);wj=1;8jin(5). The outer\\nlearning loop (5)describes the N-BEATS training procedure\\nacross tasks (TS) with no modiﬁcation.\\nIt is clear that the ﬁnal output of the architecture de-\\npends on the sequence m0:L. Even if predictor parameters\\nwg,wfare shared across blocks and ﬁxed, the behaviour\\nofPm0:L;w(\\x01) = gwg\\x0efwf;m0:L(\\x01)is governed by an extended\\nspace of parameters (w;m1;m2;:::). Therefore, the expressive\\npower of the architecture can be expected to grow with the\\ngrowing number of blocks, in proportion to the growth of\\nthe space spanned by m0:L, even if wg,wfare shared across\\nblocks. Thus, it is reasonable to expect that the addition of\\nidentical blocks will improve generalization performance\\nbecause of the increase in expressive power.\\n3.1 Linear Approximation Analysis\\nNext, we go a level deeper in the analysis to uncover more in-\\ntricate task adaptation processes. Using linear approximation\\nanalysis, we express N-BEATS’ meta-learning operation in\\nterms of the adaptation of the internal weights of the network\\nbased on the task input data. In particular, assuming small\\nbx`,(7)can be approximated using the ﬁrst order Taylor series\\nexpansion in the vicinity of x`\\x001:\\nby=g\\x0ef(x)+å\\n`>1[g\\x00Jg\\x0ef(x`\\x001)q]\\x0ef(x`\\x001)\\n+o(kq\\x0ef(x`\\x001)k):\\n9245',\n",
       " 'Here Jg\\x0ef(x`\\x001) =Jg(f(x`\\x001))Jf(x`\\x001)is the Jacobian of\\ng\\x0ef. We now consider linear gandq, as mentioned earlier,\\nin which case gandqare represented by two matrices of\\nappropriate dimensionality, GandQ; and Jg(f(x`\\x001)) = G.\\nThus, the above expression can be simpliﬁed as:\\nby=Gf(x)+å\\n`>1G[I\\x00Jf(x`\\x001)Q]f(x`\\x001)+o(kQ f(x`\\x001)k):\\nContinuously applying the linear approximation f(x`) = [I\\x00\\nJf(x`\\x001)Q]f(x`\\x001) +o(kQ f(x`\\x001)k)until we reach `=1\\nand recalling that x1\\x11xwe arrive at the following:\\nby=å\\n`>0G\"\\n`\\x001\\nÕ\\nk=1[I\\x00Jf(x`\\x00k)Q]#\\nf(x)+ o(kQ f(x`)k): (8)\\nNote that G\\x00\\nÕ`\\x001\\nk=1[I\\x00Jf(x`\\x00k)Q]\\x01\\ncan be written in the iter-\\native update form. Consider G0\\n1=G, then the update equation\\nforG0can be written as G0\\n`=G0\\n`\\x001[I\\x00Jf(x`\\x001)Q];8` > 1\\nand (8) becomes:\\nby=å\\n`>0G0\\n`f(x)+ o(kQ f(x`)k): (9)\\nLet us now discuss how (9)can be used to re-interpret N-\\nBEATS as an instance of the meta-learning framework (4)\\nand(5). The predictor can now be represented in a decoupled\\nformPq;w(\\x01) = gq\\x0efwf(\\x01). Thus task adaptation is clearly\\nconﬁned in the decision function, gq, whereas the embedding\\nfunction fwfonly relies on ﬁxed meta-parameters wf. The\\nadaptive parameters qinclude the sequence of projection\\nmatricesfG0\\n`g. The meta-initialization function It0is param-\\neterized with t0\\x11Gand it simply sets G0\\n1 t0. The main\\ningredient of the update function UuisQfwf(\\x01), parameter-\\nized as before with u= (Q;wf). The update function now\\nconsists of two equations:\\nG0\\n` G0\\n`\\x001[I\\x00Jf(x\\x00m`\\x001)Q];8` > 1;\\nm` m`\\x001+Qfwf(x\\x00m`\\x001);m0=0:(10)\\nThe ﬁrst order analysis results (9)and(10) suggest that\\nunder certain circumstances, the block-by-block manipula-\\ntion of the input sequence apparent in (7)is equivalent to\\nproducing an iterative update of predictor’s ﬁnal linear layer\\nweights apparent in (10), with the block input being set to\\nthe same ﬁxed value. This is very similar to the ﬁnal linear\\nlayer update behaviour identiﬁed in other meta-learning algo-\\nrithms: in LEO it is present by design (Rusu et al. 2019), in\\nMAML it was identiﬁed by Raghu et al. (2020), and in PNs\\nit follows from the results of our analysis in Section 2.2.\\n3.2 The Role of Q\\nIt is hard to study the form of Qlearned from the data in\\ngeneral. However, equipped with the results of the linear\\napproximation analysis presented in Section 3.1, we can study\\nthe case of a two-block network, assuming that the L2norm\\nloss between yandbyis used to train the network. If, in\\naddition, the dataset consists of the set of Npairsfxi;yigi=1\\nthe dataset-wise loss Lhas the following expression:\\nL=å\\ni\\r\\ryi\\x002Gf(xi) +Jg\\x0ef(xi)Qf(xi)+o(kQ f(xi))k)\\r\\r2:Introducing Dyi=yi\\x002Gf(xi), the error between the default\\nforecast 2Gf(xi)and the ground truth yi, and expanding the\\nL2norm we obtain the following:\\nL=å\\niDyi|Dyi+2Dyi|Jg\\x0ef(xi)Qf(xi)\\n+f(xi)|Q|J|\\ng\\x0ef(xi)Jg\\x0ef(xi)Qf(xi)+o(kQ f(xi))k):\\nNow, assuming that the rest of the parameters of the network\\nare ﬁxed, we have the derivative with respect to Qusing\\nmatrix calculus (Petersen and Pedersen 2012):\\n¶L\\n¶Q=å\\ni2J|\\ng\\x0ef(xi)Dyif(xi)|\\n+2J|\\ng\\x0ef(xi)Jg\\x0ef(xi)Qf(xi)f(xi)|+o(kQ f(xi))k):\\nUsing the above expression we conclude that the ﬁrst-order\\napproximation of optimal Qsatisﬁes the following equation:\\nå\\niJ|\\ng\\x0ef(xi)Dyif(xi)|=\\x00å\\niJ|\\ng\\x0ef(xi)Jg\\x0ef(xi)Qf(xi)f(xi)|:\\nAlthough this does not help to ﬁnd a closed form solution\\nforQ, it does provide a quite obvious intuition: the LHS\\nand the RHS are equal when the correction term created\\nby the second block, Jg\\x0ef(xi)Qf(xi), tends to compensate\\nthe default forecast error, Dyi. Therefore, Qsatisfying the\\nequation will tend to drive the update to Gin(10) in such a\\nway that on average the projection of f(x)over the update\\nJg\\x0ef(x)Q to matrix Gwill tend to compensate the error Dy\\nmade by forecasting yusing Gbased on meta-initialization.\\n3.3 Factors Enabling Meta-learning\\nLet us now analyze the factors that enable the meta-learning\\ninner loop obvious in (10). First, meta-learning regime is\\nnot viable without having multiple blocks connected via the\\nresidual connection (feedback loop): x`=x`\\x001\\x00q\\x0ef(x`\\x001).\\nSecond, the meta-learning inner loop is not viable when f\\nis linear: the update of Gis extracted from the curvature of\\nfat the point dictated by the input xand the sequence of\\nshifts m0:L. Indeed, suppose fis linear, and denote it by linear\\noperator F. The Jacobian Jf(x`\\x001)becomes a constant, F.\\nEquation (8)simpliﬁes as (note that for linear f,(8)is exact):\\nby=å\\n`>0G[I\\x00FQ]`\\x001Fx:\\nTherefore, Gå`>0[I\\x00FQ]`\\x001may be replaced with an equiv-\\nalent G0that is not data adaptive. Interestingly, å`>0[I\\x00\\nFQ]`\\x001happens to be a truncated Neumann series. Denoting\\nMoore-Penrose pseudo-inverse as [\\x01]+, assuming bounded-\\nness of FQand completing the series, å¥\\n`=0[I\\x00FQ]`, results\\ninby=G[FQ]+Fx. Therefore, under certain conditions, the\\nN-BEATS architecture with linear fand inﬁnite number of\\nblocks can be interpreted as a linear predictor of a signal in\\ncolored noise. Here the [FQ]+part cleans the intermediate\\nspace created by projection Ffrom the components that are\\nundesired for forecasting and Gcreates the forecast based on\\nthe initial projection Fxafter it is “sanitized” by [FQ]+.\\nIn this section we established that N-BEATS is an instance\\nof a meta-learning algorithm described by equations (4)and\\n9246',\n",
       " 'M4, s MAPE M3, s MAPE‡TOURISM ,MAPE ELECTR /TRAFF ,ND FRED , sMAPE\\nPure ML 12.89 Comb 13.52 ETS 20.88 MatFact 0.16 / 0.20 ETS 14.16\\nBest STAT 11.99 ForePro 13.19 Theta 20.88 DeepAR 0.07 / 0.17 Naïve 12.79\\nProLogistica 11.85 Theta 13.01 ForePro 19.84 DeepState 0.08 / 0.17 SES 12.70\\nBest ML/TS 11.72 DOTM 12.90 Strato 19.52 Theta 0.08 / 0.18 Theta 12.20\\nDL/TS hybrid 11.37 EXP 12.71 LCBaker 19.35 ARIMA 0.07 / 0.15 ARIMA 12.15\\nN-BEATS 11.14 12.37 18.52 0.07 / 0.11 11.49\\nDeepAR\\x0312.25 12.67 19.27 0.09 / 0.19 n/a\\nDeepAR-M4\\x03n/a 14.76 24.79 0.15 / 0.36 n/a\\nN-BEATS-M4 n/a 12.44 18.82 0.09 / 0.15 11.60\\nN-BEATS-FR 11.70 12.69 19.94 † 0:09 / 0:26 n/a\\nTable 1: Dataset-speciﬁc metrics aggregated over each dataset; lower values are better. The bottom three rows represent the\\nzero-shot transfer setup, indicating respectively the core algorithm (DeepAR or N-BEATS) and the source dataset (M4 or\\nFR(ED)). All other model names are explained in Appendix F.†N-BEATS trained on double upsampled monthly data, see\\nAppendix D.‡M3/M4 s MAPE deﬁnitions differ.\\x03DeepAR trained by us using GluonTS.\\n(5). We showed that each block of N-BEATS is an inner\\nmeta-learning loop that generates additional shift parameters\\nspeciﬁc to the input time series. Therefore, the expressive\\npower of the architecture is expected to grow with each ad-\\nditional block, even if all blocks share their parameters. We\\nused linear approximation analysis to show that the input\\nshift in a block is equivalent to the update of the block’s ﬁnal\\nlinear layer weights under certain conditions. The key role in\\nthis process seems to be encapsulated in the non-linearity of\\nfand in residual connections.\\n4 Empirical Results\\nWe evaluate performance on a number of datasets repre-\\nsenting a diverse set of univariate time series. For each of\\nthem, we evaluate the base N-BEATS performance compared\\nagainst the best-published approaches. We also evaluate zero-\\nshot transfer from several source datasets, as explained next.\\nBase datasets. M4 (M4 Team 2018), contains 100k TS\\nrepresenting demographic, ﬁnance, industry, macro and mi-\\ncro indicators. Sampling frequencies include yearly, quarterly,\\nmonthly, weekly, daily and hourly. M3(Makridakis and Hi-\\nbon 2000) contains 3003 TS from domains and sampling\\nfrequencies similar to M4. FRED is a dataset introduced in\\nthis paper containing 290k US and international economic\\nTS from 89 sources, a subset of the data published by the\\nFederal Reserve Bank of St. Louis (Federal Reserve 2019).\\nTOURISM (Athanasopoulos et al. 2011) includes monthly,\\nquarterly and yearly series of indicators related to tourism\\nactivities. ELECTRICITY (Dua and Graff 2017; Yu, Rao, and\\nDhillon 2016) represents the hourly electricity usage of 370\\ncustomers. TRAFFIC (Dua and Graff 2017; Yu, Rao, and\\nDhillon 2016) tracks hourly occupancy rates of 963 lanes\\nin the Bay Area freeways. Additional details for all datasets\\nappear in Appendix E.\\nZero-shot TS forecasting task deﬁnition . One of the\\nbase datasets, a source dataset, is used to train a machine\\nlearning model. The trained model then forecasts a TS in\\natarget dataset. The source and the target datasets are dis-tinct: they do not contain TS whose values are linear trans-\\nformations of each other. The forecasted TS is split into two\\nnon-overlapping pieces: the history, and the test. The history\\nis used as model input and the test is used to compute the\\nforecast error metric. We use the history and the test splits\\nfor the base datasets consistent with their original publica-\\ntion, unless explicitly stated otherwise. To produce forecasts,\\nthe model is allowed to access the TS in the target dataset\\non a one-at-a-time basis. This is to avoid having the model\\nimplicitly learn/adapt based on any information contained in\\nthe target dataset other than the history of the forecasted TS.\\nIf any adjustments of model parameters or hyperparameters\\nare necessary, they are allowed exclusively using the history\\nof the forecasted TS.\\nTraining setup. DeepAR (Salinas et al. 2019) is trained\\nusing GluonTS implementation from its authors (Alexan-\\ndrov et al. 2019). N-BEATS is trained following the original\\ntraining setup of Oreshkin et al. (2020). Both N-BEATS and\\nDeepAR are trained with scaling/descaling the architecture\\ninput/output by dividing/multiplying all input/output values\\nby the max value of the input window computed per target\\ntime-series. This does not affect the accuracy of the models\\nin the usual train/test scenario. In the zero-shot regime, this\\noperation is intended to prevent catastrophic failure when the\\nscale of the target time-series differs signiﬁcantly from those\\nof the source dataset. Additional training setup details are\\nprovided in Appendix D.\\nKey results. For each dataset, we compare our results\\nto 5 representative entries reported in the literature for that\\ndataset, based on dataset-speciﬁc metrics (M4, FRED , M3:\\nsMAPE ;TOURISM :MAPE ;ELECTRICITY ,TRAFFIC :ND). We\\nadditionally train the popular machine learning TS model\\nDeepAR and evaluate it in the zero-shot regime. Our main\\nresults appear in Table 1, with more details provided in Ap-\\npendix F. In the zero-shot forecasting regime (bottom three\\nrows), N-BEATS consistently outperforms most statistical\\nmodels tailored to these datasets as well as DeepAR trained\\non M4 and evaluated in zero-shot regime on other datasets. N-\\nBEATS trained on FRED and applied in the zero-shot regime\\n9247',\n",
       " 'Figure 1: Zero-shot forecasting performance of N-BEATS trained on M4 and applied to M3 (left) and TOURISM (right) target\\ndatasets with respect to the number of blocks, L. The mean and one standard deviation interval (based on ensemble bootstrap)\\nwith (blue) and without (red) weight sharing across blocks are shown. The extended set of results for all datasets, using FRED as\\na source dataset and a few metrics are provided in Appendix G, further reinforcing our ﬁndings.\\nto M4 outperforms the best statistical model selected for its\\nperformance on M4 and is at par with the competition’s sec-\\nond entry (boosted trees). On M3 and TOURISM the zero-shot\\nforecasting performance of N-BEATS is better than that of\\nthe M3 winner, Theta (Assimakopoulos and Nikolopoulos\\n2000). On ELECTRICITY and TRAFFIC N-BEATS performs\\nclose to or better than other neural models trained on these\\ndatasets. The results suggest that a neural model is able to\\nextract general knowledge about TS forecasting and then\\nsuccessfully adapt it to forecast on unseen TS. Our study\\npresents the ﬁrst successful application of a neural model to\\nsolve univariate zero-shot TS point forecasting across a large\\nvariety of datasets, and suggests that a pre-trained N-BEATS\\nmodel can constitute a strong baseline for this task.\\nMeta-learning Effects . Analysis in Section 3 implies that\\nN-BEATS internally generates a sequence of parameters that\\ndynamically extend the expressive power of the architecture\\nwith each newly added block, even if the blocks are identi-\\ncal. To validate this hypothesis, we performed an experiment\\nstudying the zero-shot forecasting performance of N-BEATS\\nwith increasing number of blocks, with and without param-\\neter sharing. The architecture was trained on M4 and the\\nperformance was measured on the target datasets M3 and\\nTOURISM . The results are presented in Fig. 1. On the two\\ndatasets and for the shared-weights conﬁguration, we con-\\nsistently see performance improvement when the number of\\nblocks increases up to about 30 blocks. In the same scenario,\\nincreasing the number of blocks beyond 30 leads to small,\\nbut consistent deterioration in performance. One can view\\nthese results as evidence supporting the meta-learning inter-\\npretation of N-BEATS, with a possible explanation of this\\nphenomenon as overﬁtting in the meta-learning inner loop.\\nIt would not otherwise be obvious how to explain the gener-\\nalization dynamics in Fig. 1. Additionally, the performance\\nimprovement due to meta-learning alone (shared weights,\\nmultiple blocks vs. a single block) is 12.60 to 12.44 (1.2%)\\nand 20.40 to 18.82 (7.8%) for M3 and TOURISM , respec-\\ntively (see Fig. 1). The performance improvement due to\\nmeta-learning and unique weights1(unique weights, multiple\\n1Intuitively, the network with unique block weights includes theblocks vs. a single block) is 12.60 to 12.40 (1.6%) and 20.40\\nto 18.91 (7.4%). Clearly, the majority of the gain is due to\\nthe meta-learning alone. The introduction of unique block\\nweights sometimes results in marginal gain, but often leads\\nto a loss (see more results in Appendix G).\\nIn this section, we presented empirical evidence that neu-\\nral networks are able to provide high-quality zero-shot fore-\\ncasts on unseen TS. We further empirically supported the hy-\\npothesis that meta-learning adaptation mechanisms identiﬁed\\nwithin N-BEATS in Section 3 are instrumental in achieving\\nimpressive zero-shot forecasting accuracy results.\\n5 Discussion and Conclusion\\nZero-shot transfer learning. We propose a broad meta-\\nlearning framework and explain mechanisms facilitating zero-\\nshot forecasting. Our results show that neural networks can\\nextract generic knowledge about forecasting and apply it\\nin zero-shot transfer. Residual architectures in general are\\ncovered by the analysis of Sec. 3, which might explain some\\nof the success of residual architectures, although their deeper\\nstudy should be subject to future work. Our theory suggests\\nthat residual connections generate, on-the-ﬂy, compact task-\\nspeciﬁc parameter updates by producing a sequence of input\\nshifts for identical blocks. Sec. 3.1 reinterprets our results\\nshowing that, as a ﬁrst-order approximation residual con-\\nnections produce an iterative update to the predictor ﬁnal\\nlinear layer. Memory efﬁciency and knowledge compres-\\nsion. Our empirical results imply that N-BEATS is able to\\ncompress all the relevant knowledge about a given dataset in\\na single block, rather than in 10 or 30 blocks with individ-\\nual weights. From a practical perspective, this could be used\\nto obtain 10–30 times neural network weight compression\\nand is relevant in applications where storing neural networks\\nefﬁciently is important.\\nnetwork with identical weights as a special case. Therefore, it is\\nfree to combine the effect of meta-learning with the effect of unique\\nblock weights based on its training loss.\\n9248',\n",
       " 'References\\nAlexandrov, A.; Benidis, K.; Bohlke-Schneider, M.; Flunkert, V .;\\nGasthaus, J.; Januschowski, T.; Maddix, D. C.; Rangapuram, S.;\\nSalinas, D.; Schulz, J.; Stella, L.; Türkmen, A. C.; and Wang, Y .\\n2019. GluonTS: Probabilistic Time Series Modeling in Python.\\narXiv preprint arXiv:1906.05264 .\\nAssimakopoulos, V .; and Nikolopoulos, K. 2000. The theta model:\\na decomposition approach to forecasting. International Journal of\\nForecasting 16(4): 521–530.\\nAthanasopoulos, G.; Hyndman, R. J.; Song, H.; and Wu, D. C.\\n2011. The tourism forecasting competition. International Journal\\nof Forecasting 27(3): 822–844.\\nBengio, S.; Bengio, Y .; Cloutier, J.; and Gecsei, J. 1992. On the\\noptimization of a synaptic learning rule. In Optimality in Artiﬁcial\\nand Biological Neural Networks.\\nBengio, Y .; Bengio, S.; and Cloutier, J. 1991. Learning a Synaptic\\nLearning Rule. In Proceedings of the International Joint Conference\\non Neural Networks, II–A969. Seattle, USA.\\nChapados, N.; Joliveau, M.; L’Écuyer, P.; and Rousseau, L.-M. 2014.\\nRetail store scheduling for proﬁt. European Journal of Operational\\nResearch 239(3): 609 – 624.\\nDua, D.; and Graff, C. 2017. UCI Machine Learning Repository.\\nURL http://archive.ics.uci.edu/ml. Last Accessed: 2021-03-01.\\nEngle, R. F. 1982. Autoregressive conditional heteroscedasticity\\nwith estimates of the variance of United Kingdom inﬂation. Econo-\\nmetrica 50(4): 987–1007.\\nFawaz, H. I.; Forestier, G.; Weber, J.; Idoumghar, L.; and Muller,\\nP.-A. 2018. Transfer learning for time series classiﬁcation. 2018\\nIEEE International Conference on Big Data (Big Data) .\\nFederal Reserve Bank of St. Louis. 2019. FRED Economic Data.\\nData retrieved from https://fred.stlouisfed.org/ Accessed: 2019-11-\\n01.\\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-Agnostic Meta-\\nLearning for Fast Adaptation of Deep Networks. In ICML, 1126–\\n1135.\\nFiorucci, J. A.; Pellegrini, T. R.; Louzada, F.; Petropoulos, F.; and\\nKoehler, A. B. 2016. Models for optimising the theta method and\\ntheir relationship to state space models. International Journal of\\nForecasting 32(4): 1151–1161.\\nGao, J. 2014. Machine learning applications for data center opti-\\nmization. Technical report, Google.\\nGauss, C. F. 1809. Theoria motus corporum coelestium in section-\\nibus conicis solem ambientium. Hamburg: Frid. Perthes and I. H.\\nBesser.\\nHarlow, H. F. 1949. The Formation of Learning Sets. Psychological\\nReview 56(1): 51–65. doi:10.1037/h0062474.\\nHolt, C. C. 1957. Forecasting trends and seasonals by exponentially\\nweighted averages. Technical Report ONR memorandum no. 5,\\nCarnegie Institute of Technology, Pittsburgh, PA.\\nHolt, C. C. 2004. Forecasting seasonals and trends by exponentially\\nweighted moving averages. International Journal of Forecasting\\n20(1): 5–10.\\nHooshmand, A.; and Sharma, R. 2019. Energy Predictive Models\\nwith Limited Data Using Transfer Learning. In Proceedings of the\\nTenth ACM International Conference on Future Energy Systems,\\ne-Energy’19, 12–16.Hyndman, R. J.; and Khandakar, Y . 2008. Automatic time series\\nforecasting: the forecast package for R. Journal of Statistical Soft-\\nware 26(3): 1–22.\\nIbrahim, R.; Ye, H.; L’Ecuyer, P.; and Shen, H. 2016. Modeling and\\nforecasting call center arrivals: A literature survey and a case study.\\nInternational Journal of Forecasting 32(3): 865–874.\\nKahn, K. B. 2003. How to Measure the Impact of a Forecast Error\\non an Enterprise? The Journal of Business Forecasting Methods &\\nSystems 22(1).\\nKerkkänen, A.; Korpela, J.; and Huiskonen, J. 2009. Demand\\nforecasting errors in industrial context: Measurement and impacts.\\nInternational Journal of Production Economics 118(1): 43–48.\\nLee, Y .; and Choi, S. 2018. Gradient-based meta-learning with\\nlearned layerwise metric and subspace. In ICML, 2933–2942.\\nLeung, H. C. 1995. Neural networks in supply chain management.\\nInProceedings for Operating Research and the Management Sci-\\nences, 347–352.\\nLi, Z.; Zhou, F.; Chen, F.; and Li, H. 2017. Meta-SGD: Learning to\\nLearn Quickly for Few Shot Learning. CoRR abs/1707.09835.\\nM4 Team. 2018. M4 competitor’s guide: prizes and rules.\\nURL https://usermanual.wiki/Document/M4CompetitorsGuide.\\n1491768831/view. Last Accessed: 2021-03-01.\\nMakridakis, S.; Andersen, A.; Carbone, R.; Fildes, R.; Hibon, M.;\\nLewandowski, R.; Newton, J.; Parzen, E.; and Winkler, R. 1982.\\nThe accuracy of extrapolation (time series) methods: Results of a\\nforecasting competition. Journal of forecasting 1(2): 111–153.\\nMakridakis, S.; Chatﬁeld, C.; Hibon, M.; Lawrence, M.; Mills, T.;\\nOrd, K.; and Simmons, L. F. 1993. The M2-competition: A real-\\ntime judgmentally based forecasting study. International Journal of\\nForecasting 9(1): 5–22.\\nMakridakis, S.; and Hibon, M. 2000. The M3-Competition: results,\\nconclusions and implications. International Journal of Forecasting\\n16(4): 451–476.\\nMakridakis, S.; Spiliotis, E.; and Assimakopoulos, V . 2018a. The\\nM4-Competition: Results, ﬁndings, conclusion and way forward.\\nInternational Journal of Forecasting 34(4): 802–808.\\nMakridakis, S.; Spiliotis, E.; and Assimakopoulos, V . 2018b. Sta-\\ntistical and Machine Learning forecasting methods: Concerns and\\nways forward. PLoS ONE 13(3).\\nMontero-Manso, P.; Athanasopoulos, G.; Hyndman, R. J.; and Tala-\\ngala, T. S. 2020. FFORMA: Feature-based forecast model averaging.\\nInternational Journal of Forecasting 36(1): 86–92.\\nNeale, A. A. 1985. Weather Forecasting: Magic, Art, Science and\\nHypnosis. Weather and Climate 5(1): 2–5.\\nNguyen, H.-N.; Ni, Q.; and Rossetti, M. D. 2010. Exploring the\\ncost of forecast error in inventory systems. In Proceedings of the\\n2010 Industrial Engineering Research Conference.\\nOreshkin, B. N.; Carpov, D.; Chapados, N.; and Bengio, Y . 2020.\\nN-BEATS: Neural basis expansion analysis for interpretable time\\nseries forecasting. In ICLR.\\nPetersen, K. B.; and Pedersen, M. S. 2012. The Matrix Cookbook.\\nVersion 20121115.\\nRaghu, A.; Raghu, M.; Bengio, S.; and Vinyals, O. 2020. Rapid\\nLearning or Feature Reuse? Towards Understanding the Effective-\\nness of MAML. In International Conference on Learning Repre-\\nsentations.\\n9249',\n",
       " 'Rangapuram, S. S.; Seeger, M.; Gasthaus, J.; Stella, L.; Wang, Y .;\\nand Januschowski, T. 2018. Deep State Space Models for Time\\nSeries Forecasting. In NeurIPS.\\nRavi, S.; and Larochelle, H. 2016. Optimization as a model for\\nfew-shot learning. In ICLR.\\nRibeiro, M.; Grolinger, K.; ElYamany, H. F.; Higashino, W. A.;\\nand Capretz, M. A. 2018. Transfer learning with seasonal and\\ntrend adjustment for cross-building energy forecasting. Energy and\\nBuildings 165: 352–363.\\nRodrigues Jr, F. A.; Jabloun, M.; Ortiz-Monasterio, J. I.; Crout, N.\\nM. J.; Gurusamy, S.; and Green, S. 2019. Mexican Crop Obser-\\nvation, Management and Production Analysis Services System —\\nCOMPASS. In Poster Proceedings of the 12th European Conference\\non Precision Agriculture.\\nRusu, A. A.; Rao, D.; Sygnowski, J.; Vinyals, O.; Pascanu, R.;\\nOsindero, S.; and Hadsell, R. 2019. Meta-Learning with Latent\\nEmbedding Optimization. In ICLR.\\nSalinas, D.; Flunkert, V .; Gasthaus, J.; and Januschowski, T. 2019.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent\\nnetworks. International Journal of Forecasting .\\nSchmidhuber, J. 1987. Evolutionary principles in self-referential\\nlearning. Master’s thesis, Institut f. Informatik, Tech. Univ. Munich.\\nSezer, O. B.; Gudelek, M. U.; and Ozbayoglu, A. M. 2019. Finan-\\ncial Time Series Forecasting with Deep Learning : A Systematic\\nLiterature Review: 2005-2019.\\nSmyl, S. 2020. A hybrid method of exponential smoothing and\\nrecurrent neural networks for time series forecasting. International\\nJournal of Forecasting 36(1): 75 – 85.\\nSnell, J.; Swersky, K.; and Zemel, R. S. 2017. Prototypical Networks\\nfor Few-shot Learning. In NIPS, 4080–4090.\\nSpiliotis, E.; Assimakopoulos, V .; and Nikolopoulos, K. 2019. Fore-\\ncasting with a hybrid method utilizing data smoothing, a variation of\\nthe Theta method and shrinkage of seasonal factors. International\\nJournal of Production Economics 209: 92–102.\\nTang, C.; and Salakhutdinov, R. R. 2019. Multiple Futures Predic-\\ntion. In NeurIPS 32, 15398–15408.\\nVinyals, O.; Blundell, C.; Lillicrap, T.; Kavukcuoglu, K.; and Wier-\\nstra, D. 2016. Matching Networks for One Shot Learning. In NIPS,\\n3630–3638.\\nWalker, G. 1931. On Periodicity in Series of Related Terms. Proc.\\nR. Soc. Lond. A 131: 518–532.\\nWang, Y .; Smola, A.; Maddix, D. C.; Gasthaus, J.; Foster, D.; and\\nJanuschowski, T. 2019. Deep Factors for Forecasting. In ICML.\\nWinters, P. R. 1960. Forecasting Sales by Exponentially Weighted\\nMoving Averages. Management Science 6(3): 324–342.\\nYu, H.-F.; Rao, N.; and Dhillon, I. S. 2016. Temporal Regularized\\nMatrix Factorization for High-dimensional Time Series Prediction.\\nInNIPS.\\nYule, G. U. 1927. On a Method of Investigating Periodicities in Dis-\\nturbed Series, with Special Reference to Wolfer’s Sunspot Numbers.\\nPhil. Trans. the R. Soc. Lond. A 226: 267–298.\\n9250']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T06:00:02.240703Z",
     "start_time": "2024-05-30T06:00:02.233696Z"
    }
   },
   "cell_type": "code",
   "source": "len(pdf_text)",
   "id": "eaeeb6257a4b6bb8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a3ff46a375abf66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
